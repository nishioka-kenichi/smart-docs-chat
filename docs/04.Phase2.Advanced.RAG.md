# Phase 2: Advanced RAG実装ガイド

## 📌 概要

Phase 2では、基本的なRAGシステムを大幅に強化し、検索精度90%以上を目指します。

## 🎯 Day 1の目標

### 実装タスク
- [ ] HyDE（Hypothetical Document Embeddings）の実装
- [ ] RAG-Fusionの実装
- [ ] Rerankerの統合
- [ ] 統合テストとベンチマーク

### 成果物
- `phase02-advanced-rag/src/hyde.py`
- `phase02-advanced-rag/src/rag_fusion.py`
- `phase02-advanced-rag/src/reranker.py`
- `phase02-advanced-rag/src/advanced_rag_chain.py`

---

## 🔧 実装詳細

### 1. HyDE実装

#### 概念
ユーザーの質問から仮想的な回答を生成し、それを使って類似文書を検索する手法。

#### 実装コード
```python
# phase02-advanced-rag/src/hyde.py

from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from typing import List, Optional
from langchain.schema import Document

class HyDE:
    """Hypothetical Document Embeddings実装"""
    
    def __init__(self, llm: Optional[ChatOpenAI] = None):
        self.llm = llm or ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.7
        )
        
        self.hyde_prompt = PromptTemplate(
            input_variables=["question"],
            template="""
            以下の質問に対する詳細で具体的な回答を書いてください。
            この回答は実際のドキュメントに含まれている可能性のある内容として書いてください。
            
            質問: {question}
            
            回答:
            """
        )
    
    def generate_hypothetical_answer(self, question: str) -> str:
        """質問から仮想的な回答を生成"""
        chain = self.hyde_prompt | self.llm
        response = chain.invoke({"question": question})
        return response.content
    
    def search_with_hyde(
        self,
        question: str,
        vectorstore,
        k: int = 10
    ) -> List[Document]:
        """HyDEを使用した検索"""
        # 仮想回答を生成
        hypothetical_answer = self.generate_hypothetical_answer(question)
        
        # 仮想回答で検索
        results = vectorstore.similarity_search(
            hypothetical_answer,
            k=k
        )
        
        return results
```

#### 使用例
```python
# 使用例
from src.hyde import HyDE
from langchain_chroma import Chroma

hyde = HyDE()
vectorstore = Chroma(persist_directory="./chroma_db")

question = "LangGraphのステート管理について教えてください"
results = hyde.search_with_hyde(question, vectorstore)

for doc in results[:3]:
    print(f"Score: {doc.metadata.get('score', 'N/A')}")
    print(f"Content: {doc.page_content[:200]}...\n")
```

---

### 2. RAG-Fusion実装

#### 概念
複数の関連クエリを生成し、並列検索の結果を統合する手法。

#### 実装コード
```python
# phase02-advanced-rag/src/rag_fusion.py

from typing import List, Dict, Tuple
from langchain.schema import Document
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
import asyncio
from collections import defaultdict

class RAGFusion:
    """RAG-Fusion実装"""
    
    def __init__(self, llm: Optional[ChatOpenAI] = None):
        self.llm = llm or ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.7
        )
        
        self.query_generation_prompt = PromptTemplate(
            input_variables=["original_query"],
            template="""
            以下の質問に関連する、異なる観点からの検索クエリを5つ生成してください。
            各クエリは異なる側面や言い換えを含むようにしてください。
            
            元の質問: {original_query}
            
            検索クエリ（1行に1つ）:
            """
        )
    
    def generate_queries(self, original_query: str) -> List[str]:
        """元のクエリから複数の関連クエリを生成"""
        chain = self.query_generation_prompt | self.llm
        response = chain.invoke({"original_query": original_query})
        
        # レスポンスを行ごとに分割してクエリリストを作成
        queries = [q.strip() for q in response.content.split('\n') if q.strip()]
        queries.append(original_query)  # 元のクエリも含める
        
        return queries[:6]  # 最大6個まで
    
    def reciprocal_rank_fusion(
        self,
        results_list: List[List[Document]],
        k: int = 60
    ) -> List[Document]:
        """Reciprocal Rank Fusionによる結果統合"""
        # 文書のスコアを計算
        doc_scores = defaultdict(float)
        doc_map = {}
        
        for results in results_list:
            for rank, doc in enumerate(results):
                # 文書IDを生成（内容のハッシュを使用）
                doc_id = hash(doc.page_content)
                
                # RRFスコアを計算
                doc_scores[doc_id] += 1.0 / (rank + k)
                
                # 文書を保存
                if doc_id not in doc_map:
                    doc_map[doc_id] = doc
        
        # スコアでソート
        sorted_docs = sorted(
            doc_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )
        
        # Document オブジェクトのリストを返す
        return [doc_map[doc_id] for doc_id, _ in sorted_docs]
    
    async def search_with_fusion(
        self,
        query: str,
        vectorstore,
        k: int = 10
    ) -> List[Document]:
        """RAG-Fusionを使用した検索"""
        # 複数のクエリを生成
        queries = self.generate_queries(query)
        print(f"生成されたクエリ: {queries}")
        
        # 並列検索を実行
        tasks = [
            asyncio.create_task(
                asyncio.to_thread(
                    vectorstore.similarity_search,
                    q,
                    k=k
                )
            )
            for q in queries
        ]
        
        results_list = await asyncio.gather(*tasks)
        
        # 結果を統合
        fused_results = self.reciprocal_rank_fusion(results_list)
        
        return fused_results[:k]
```

---

### 3. Reranker実装

#### 概念
Cross-EncoderやCohere Rerankを使用して検索結果を再順位付け。

#### 実装コード
```python
# phase02-advanced-rag/src/reranker.py

from typing import List, Tuple, Optional
from langchain.schema import Document
import cohere
from sentence_transformers import CrossEncoder
import torch

class Reranker:
    """検索結果の再順位付け"""
    
    def __init__(
        self,
        method: str = "cross-encoder",
        cohere_api_key: Optional[str] = None
    ):
        """
        Args:
            method: "cohere" または "cross-encoder"
            cohere_api_key: Cohere APIキー（Cohereを使用する場合）
        """
        self.method = method
        
        if method == "cohere":
            if not cohere_api_key:
                raise ValueError("Cohere APIキーが必要です")
            self.cohere_client = cohere.Client(cohere_api_key)
            self.model_name = "rerank-multilingual-v3.0"
        
        elif method == "cross-encoder":
            # 日本語対応のCross-Encoderモデル
            self.cross_encoder = CrossEncoder(
                "amberoad/bert-multilingual-passage-reranking-msmarco",
                max_length=512
            )
    
    def rerank_with_cohere(
        self,
        query: str,
        documents: List[Document],
        top_k: Optional[int] = None
    ) -> List[Tuple[Document, float]]:
        """Cohere Rerankを使用した再順位付け"""
        # ドキュメントのテキストを抽出
        doc_texts = [doc.page_content for doc in documents]
        
        # Cohere Rerank APIを呼び出し
        results = self.cohere_client.rerank(
            query=query,
            documents=doc_texts,
            model=self.model_name,
            top_n=top_k if top_k else len(documents)
        )
        
        # 結果を整形
        reranked = [
            (documents[r.index], r.relevance_score)
            for r in results.results
        ]
        
        return reranked
    
    def rerank_with_cross_encoder(
        self,
        query: str,
        documents: List[Document],
        top_k: Optional[int] = None
    ) -> List[Tuple[Document, float]]:
        """Cross-Encoderを使用した再順位付け"""
        # クエリとドキュメントのペアを作成
        pairs = [[query, doc.page_content] for doc in documents]
        
        # スコアを計算
        scores = self.cross_encoder.predict(pairs)
        
        # スコアでソート
        doc_score_pairs = list(zip(documents, scores))
        doc_score_pairs.sort(key=lambda x: x[1], reverse=True)
        
        # top_kが指定されている場合は上位k個を返す
        if top_k:
            doc_score_pairs = doc_score_pairs[:top_k]
        
        return doc_score_pairs
    
    def rerank(
        self,
        query: str,
        documents: List[Document],
        top_k: Optional[int] = None
    ) -> List[Document]:
        """選択された手法で再順位付けを実行"""
        if self.method == "cohere":
            reranked = self.rerank_with_cohere(query, documents, top_k)
        else:
            reranked = self.rerank_with_cross_encoder(query, documents, top_k)
        
        # Documentのリストとして返す（スコアはmetadataに追加）
        result_docs = []
        for doc, score in reranked:
            doc.metadata["rerank_score"] = float(score)
            result_docs.append(doc)
        
        return result_docs
```

---

### 4. 統合Advanced RAGチェーン

#### 実装コード
```python
# phase02-advanced-rag/src/advanced_rag_chain.py

from typing import Dict, List, Optional
from langchain.schema import Document
from langchain_openai import ChatOpenAI
from langchain_chroma import Chroma
from src.hyde import HyDE
from src.rag_fusion import RAGFusion
from src.reranker import Reranker
import asyncio

class AdvancedRAGChain:
    """HyDE、RAG-Fusion、Rerankerを統合したAdvanced RAG"""
    
    def __init__(
        self,
        vectorstore_path: str = "./chroma_db",
        cohere_api_key: Optional[str] = None
    ):
        # コンポーネントの初期化
        self.vectorstore = Chroma(
            persist_directory=vectorstore_path,
            embedding_function=OpenAIEmbeddings(
                model="text-embedding-3-small"
            )
        )
        
        self.llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0
        )
        
        self.hyde = HyDE(self.llm)
        self.rag_fusion = RAGFusion(self.llm)
        self.reranker = Reranker(
            method="cross-encoder" if not cohere_api_key else "cohere",
            cohere_api_key=cohere_api_key
        )
    
    async def advanced_search(
        self,
        query: str,
        use_hyde: bool = True,
        use_fusion: bool = True,
        use_rerank: bool = True,
        k: int = 10
    ) -> List[Document]:
        """Advanced RAG検索の実行"""
        
        all_results = []
        
        # HyDE検索
        if use_hyde:
            print("[HyDE] 仮想回答を生成して検索中...")
            hyde_results = self.hyde.search_with_hyde(
                query, self.vectorstore, k=k*2
            )
            all_results.extend(hyde_results)
        
        # RAG-Fusion検索
        if use_fusion:
            print("[RAG-Fusion] 複数クエリで並列検索中...")
            fusion_results = await self.rag_fusion.search_with_fusion(
                query, self.vectorstore, k=k*2
            )
            all_results.extend(fusion_results)
        
        # 通常の検索も追加
        if not use_hyde and not use_fusion:
            print("[Standard] 通常検索中...")
            standard_results = self.vectorstore.similarity_search(
                query, k=k*2
            )
            all_results.extend(standard_results)
        
        # 重複を削除
        unique_docs = []
        seen_contents = set()
        for doc in all_results:
            if doc.page_content not in seen_contents:
                unique_docs.append(doc)
                seen_contents.add(doc.page_content)
        
        # Reranking
        if use_rerank and len(unique_docs) > 0:
            print(f"[Reranker] {len(unique_docs)}件の結果を再順位付け中...")
            reranked_docs = self.reranker.rerank(
                query, unique_docs, top_k=k
            )
            return reranked_docs
        
        return unique_docs[:k]
    
    def generate_answer(
        self,
        query: str,
        documents: List[Document]
    ) -> str:
        """検索結果を基に回答を生成"""
        # コンテキストの作成
        context = "\n\n".join([
            f"[文書{i+1}]\n{doc.page_content}"
            for i, doc in enumerate(documents)
        ])
        
        # プロンプトの作成
        prompt = f"""
        以下のコンテキストを参考に、質問に対して詳細かつ正確に回答してください。
        
        コンテキスト:
        {context}
        
        質問: {query}
        
        回答:
        """
        
        # 回答生成
        response = self.llm.invoke(prompt)
        return response.content
    
    async def ask(
        self,
        query: str,
        use_hyde: bool = True,
        use_fusion: bool = True,
        use_rerank: bool = True
    ) -> Dict:
        """質問に対する回答を生成"""
        # Advanced検索
        documents = await self.advanced_search(
            query,
            use_hyde=use_hyde,
            use_fusion=use_fusion,
            use_rerank=use_rerank
        )
        
        # 回答生成
        answer = self.generate_answer(query, documents)
        
        return {
            "query": query,
            "answer": answer,
            "sources": documents,
            "techniques_used": {
                "hyde": use_hyde,
                "fusion": use_fusion,
                "rerank": use_rerank
            }
        }
```

---

## 🧪 テストとベンチマーク

### 精度測定スクリプト
```python
# phase02-advanced-rag/test_advanced_rag.py

import asyncio
from src.advanced_rag_chain import AdvancedRAGChain
import time

async def benchmark_advanced_rag():
    """Advanced RAGのベンチマークテスト"""
    
    # テスト用の質問
    test_queries = [
        "LangGraphのStateGraphとMessageGraphの違いは？",
        "RAGシステムでチャンクサイズはどう決めるべきか？",
        "エージェントのメモリ管理について教えて",
        "HyDEとRAG-Fusionを組み合わせる利点は？",
        "マルチエージェントシステムのアーキテクチャは？"
    ]
    
    # Advanced RAGチェーンの初期化
    rag = AdvancedRAGChain()
    
    print("=" * 60)
    print("Advanced RAG ベンチマークテスト")
    print("=" * 60)
    
    for i, query in enumerate(test_queries, 1):
        print(f"\n[テスト {i}/{len(test_queries)}]")
        print(f"質問: {query}")
        print("-" * 40)
        
        start_time = time.time()
        
        # Advanced RAGで回答生成
        result = await rag.ask(query)
        
        elapsed_time = time.time() - start_time
        
        print(f"回答: {result['answer'][:200]}...")
        print(f"処理時間: {elapsed_time:.2f}秒")
        print(f"使用技術: {result['techniques_used']}")
        print(f"取得文書数: {len(result['sources'])}")
    
    print("\n" + "=" * 60)
    print("ベンチマーク完了")
    print("=" * 60)

# 実行
if __name__ == "__main__":
    asyncio.run(benchmark_advanced_rag())
```

---

## 📊 期待される結果

### パフォーマンス比較

| 手法 | 検索精度 | 応答時間 | 特徴 |
|------|---------|---------|------|
| Phase 1 (基本RAG) | 69-88% | 8-12秒 | シンプル、高速 |
| HyDE | 80-90% | 10-15秒 | 曖昧な質問に強い |
| RAG-Fusion | 85-92% | 15-20秒 | 包括的な検索 |
| Reranker | 85-93% | +2-3秒 | 精度向上 |
| **統合版** | **90-95%** | **12-18秒** | **最高精度** |

### 改善ポイント
1. **検索精度**: 10-15%向上
2. **回答の網羅性**: 複数観点からの情報統合
3. **ロバスト性**: 様々な質問形式に対応

---

## 🚀 実行手順

### 1. 環境セットアップ
```bash
cd phase02-advanced-rag
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### 2. 必要なパッケージ
```txt
# requirements.txt
langchain>=0.3.0
langchain-openai>=0.2.0
langchain-chroma>=0.1.0
chromadb>=0.5.0
cohere>=5.0.0
sentence-transformers>=3.0.0
python-dotenv>=1.0.0
```

### 3. 環境変数設定
```bash
# .env
OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxx
COHERE_API_KEY=xxxxxxxxxxxxxxxxxxxx  # オプション
```

### 4. 実行
```bash
# メインアプリケーション実行
python main.py

# ベンチマークテスト
python test_advanced_rag.py
```

---

## 📝 Day 1のチェックリスト

- [ ] 午前: HyDE実装完了
- [ ] 午前: RAG-Fusion実装完了
- [ ] 午後: Reranker統合完了
- [ ] 午後: 統合テスト実施
- [ ] 夕方: ベンチマーク測定
- [ ] 夜: ドキュメント更新

---

## 🎯 成功基準

1. **検索精度90%以上達成**
2. **3つの技術すべて統合成功**
3. **ベンチマークテスト完了**
4. **Phase 3への準備完了**

---

*最終更新: 2025年1月21日*